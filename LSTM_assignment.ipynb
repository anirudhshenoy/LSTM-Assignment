{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.layers import Input, Dense, Embedding, LSTM, concatenate, Flatten, Dropout\n",
    "from keras.utils import np_utils \n",
    "from keras.datasets import mnist \n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adadelta\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.text import text_to_word_sequence, Tokenizer, one_hot\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np \n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>school_state</th>\n",
       "      <th>teacher_prefix</th>\n",
       "      <th>project_grade_category</th>\n",
       "      <th>teacher_number_of_previously_posted_projects</th>\n",
       "      <th>project_is_approved</th>\n",
       "      <th>clean_categories</th>\n",
       "      <th>clean_subcategories</th>\n",
       "      <th>essay</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ca</td>\n",
       "      <td>mrs</td>\n",
       "      <td>grades_prek_2</td>\n",
       "      <td>53</td>\n",
       "      <td>1</td>\n",
       "      <td>math_science</td>\n",
       "      <td>appliedsciences health_lifescience</td>\n",
       "      <td>i fortunate enough use fairy tale stem kits cl...</td>\n",
       "      <td>725.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ut</td>\n",
       "      <td>ms</td>\n",
       "      <td>grades_3_5</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>specialneeds</td>\n",
       "      <td>specialneeds</td>\n",
       "      <td>imagine 8 9 years old you third grade classroo...</td>\n",
       "      <td>213.03</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  school_state teacher_prefix project_grade_category  \\\n",
       "0           ca            mrs          grades_prek_2   \n",
       "1           ut             ms             grades_3_5   \n",
       "\n",
       "   teacher_number_of_previously_posted_projects  project_is_approved  \\\n",
       "0                                            53                    1   \n",
       "1                                             4                    1   \n",
       "\n",
       "  clean_categories                 clean_subcategories  \\\n",
       "0     math_science  appliedsciences health_lifescience   \n",
       "1     specialneeds                        specialneeds   \n",
       "\n",
       "                                               essay   price  \n",
       "0  i fortunate enough use fairy tale stem kits cl...  725.05  \n",
       "1  imagine 8 9 years old you third grade classroo...  213.03  "
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('preprocessed_data.csv')\n",
    "data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = data['project_is_approved']\n",
    "X = data.drop('project_is_approved', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, stratify = Y, random_state = 42, test_size = 0.1)\n",
    "X_train, X_cv, y_train, y_cv = train_test_split(X_train, y_train, stratify = y_train, random_state = 42, test_size = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Size : (88490, 8)\n",
      "CV Size : (9833, 8)\n",
      "Test Size : (10925, 8)\n"
     ]
    }
   ],
   "source": [
    "print('Train Size :', X_train.shape)\n",
    "print('CV Size :', X_cv.shape)\n",
    "print('Test Size :', X_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode Essays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('glove_vectors', 'rb') as f:\n",
    "    model = pickle.load(f)\n",
    "    glove_words = set(model.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "tokenizer = Tokenizer(num_words=5000)\n",
    "X_train_essays = X_train['essay'].values\n",
    "\n",
    "tokenizer.fit_on_texts(X_train_essays)\n",
    "\n",
    "train_sequences = tokenizer.texts_to_sequences(X_train_essays)\n",
    "cv_sequences = tokenizer.texts_to_sequences(X_cv['essay'].values)\n",
    "test_sequences = tokenizer.texts_to_sequences(X_test['essay'].values)\n",
    "\n",
    "\n",
    "MAX_SEQUENCE_LENGTH = np.max([len(essay) for essay in train_sequences])\n",
    "\n",
    "X_padded_essays_train = pad_sequences(train_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "X_padded_essays_cv = pad_sequences(cv_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "X_padded_essays_test = pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "107a9ebd910b485183449e498bef986e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=51964), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm_notebook\n",
    "GLOVE_VECTOR_DIMENSION = 300\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, GLOVE_VECTOR_DIMENSION))\n",
    "for word, i in tqdm_notebook(word_index.items()):\n",
    "    embedding_vector = model.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the LSTM Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_school_state = X_train['school_state'].values\n",
    "state_vocab = list(set(state for state in X_train_school_state ))\n",
    "vocab_size = len(state_vocab)\n",
    "\n",
    "X_state_train = [one_hot(state, vocab_size) for state in X_train_school_state]\n",
    "X_state_cv = [one_hot(state, vocab_size) for state in X_cv['school_state'].values]\n",
    "X_state_test = [one_hot(state, vocab_size) for state in X_test['school_state'].values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_project_grade_category = X_train['project_grade_category'].values\n",
    "grade_vocab = list(set(grade for grade in X_train_project_grade_category ))\n",
    "vocab_size = len(grade_vocab)\n",
    "\n",
    "X_grade_train = [one_hot(grade, vocab_size, filters = '') for grade in X_train_project_grade_category]\n",
    "X_grade_cv = [one_hot(grade, vocab_size, filters = '') for grade in X_cv['project_grade_category'].values]\n",
    "X_grade_test = [one_hot(grade, vocab_size, filters = '') for grade in X_test['project_grade_category'].values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_categories = X_train['clean_categories'].values\n",
    "category_vocab = list(set(category for categories in X_train_categories for category in categories.split()))\n",
    "vocab_size = len(category_vocab)\n",
    "\n",
    "X_category_train = [one_hot(category, vocab_size, filters = '') for category in X_train_categories]\n",
    "X_category_cv = [one_hot(category, vocab_size, filters = '') for category in X_cv['clean_categories'].values]\n",
    "X_category_test = [one_hot(category, vocab_size, filters = '') for category in X_test['clean_categories'].values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH_CATEGORY = np.max([len(essay) for essay in X_category_train])\n",
    "\n",
    "X_category_train = pad_sequences(X_category_train, maxlen=MAX_SEQUENCE_LENGTH_CATEGORY)\n",
    "X_category_cv = pad_sequences(X_category_cv, maxlen=MAX_SEQUENCE_LENGTH_CATEGORY)\n",
    "X_category_test = pad_sequences(X_category_test, maxlen=MAX_SEQUENCE_LENGTH_CATEGORY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_sub_categories = X_train['clean_subcategories'].values\n",
    "sub_category_vocab = list(set(category for categories in X_train_sub_categories for category in categories.split()))\n",
    "vocab_size = len(sub_category_vocab)\n",
    "\n",
    "X_sub_category_train = [one_hot(category, vocab_size, filters = '') for category in X_train_sub_categories]\n",
    "X_sub_category_cv = [one_hot(category, vocab_size, filters = '') for category in X_cv['clean_subcategories'].values]\n",
    "X_sub_category_test = [one_hot(category, vocab_size, filters = '') for category in X_test['clean_subcategories'].values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH_SUB_CATEGORY = np.max([len(essay) for essay in X_sub_category_train])\n",
    "\n",
    "X_sub_category_train = pad_sequences(X_sub_category_train, maxlen=MAX_SEQUENCE_LENGTH_SUB_CATEGORY)\n",
    "X_sub_category_cv = pad_sequences(X_sub_category_cv, maxlen=MAX_SEQUENCE_LENGTH_SUB_CATEGORY)\n",
    "X_sub_category_test = pad_sequences(X_sub_category_test, maxlen=MAX_SEQUENCE_LENGTH_SUB_CATEGORY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_teacher = X_train['teacher_prefix'].values\n",
    "teacher_vocab = list(set(category for categories in X_train_teacher for category in categories.split()))\n",
    "vocab_size = len(teacher_vocab)\n",
    "\n",
    "X_teacher_train = [one_hot(prefix, vocab_size, filters = '') for prefix in X_train_teacher]\n",
    "X_teacher_cv = [one_hot(prefix, vocab_size, filters = '') for prefix in X_cv['teacher_prefix'].values]\n",
    "X_teacher_test = [one_hot(prefix, vocab_size, filters = '') for prefix in X_test['teacher_prefix'].values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_numeric = X_train[['teacher_number_of_previously_posted_projects', 'price']].values\n",
    "X_cv_numeric = X_cv[['teacher_number_of_previously_posted_projects', 'price']].values\n",
    "X_test_numeric = X_test[['teacher_number_of_previously_posted_projects', 'price']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ESSAYS LAYER\n",
    "input_essays = Input(shape=(MAX_SEQUENCE_LENGTH,))\n",
    "x_essays = Embedding(len(word_index) + 1, \n",
    "              GLOVE_VECTOR_DIMENSION, \n",
    "              weights=[embedding_matrix],\n",
    "              input_length=MAX_SEQUENCE_LENGTH,\n",
    "              trainable = False)(input_essays)\n",
    "x_essays = LSTM(10)(x_essays)\n",
    "lstm_essay_model = Model(input_essays, x_essays)\n",
    "\n",
    "\n",
    "EMBEDDING_DIMENSION = 50\n",
    "\n",
    "# STATES LAYER\n",
    "input_states = Input(shape=(1,))\n",
    "x_state = Embedding(len(state_vocab) + 1, EMBEDDING_DIMENSION, input_length=1)(input_states)\n",
    "x_state = Flatten()(x_state)\n",
    "state_model = Model(input_states, x_state)\n",
    "\n",
    "# GRADE LAYER\n",
    "input_grades = Input(shape=(1,))\n",
    "x_grade = Embedding(len(grade_vocab) + 1, EMBEDDING_DIMENSION, input_length=1)(input_grades)\n",
    "x_grade = Flatten()(x_grade)\n",
    "grade_model = Model(input_grades, x_grade)\n",
    "\n",
    "# CATEGORY LAYER\n",
    "input_category = Input(shape=(MAX_SEQUENCE_LENGTH_CATEGORY,))\n",
    "x_category = Embedding(len(category_vocab) + 1, EMBEDDING_DIMENSION, input_length=MAX_SEQUENCE_LENGTH_CATEGORY)(input_category)\n",
    "x_category = Flatten()(x_category)\n",
    "category_model = Model(input_category, x_category)\n",
    "\n",
    "# SUB CATEGORY LAYER\n",
    "input_sub_category = Input(shape=(MAX_SEQUENCE_LENGTH_SUB_CATEGORY,))\n",
    "x_sub_category = Embedding(len(sub_category_vocab) + 1, EMBEDDING_DIMENSION, input_length=MAX_SEQUENCE_LENGTH_SUB_CATEGORY)(input_sub_category)\n",
    "x_sub_category = Flatten()(x_sub_category)\n",
    "sub_category_model = Model(input_sub_category, x_sub_category)\n",
    "\n",
    "# TEACHER PREFIX LAYER\n",
    "input_teacher = Input(shape=(1,))\n",
    "x_teacher = Embedding(len(teacher_vocab) + 1, EMBEDDING_DIMENSION, input_length=1)(input_teacher)\n",
    "x_teacher = Flatten()(x_teacher)\n",
    "teacher_model = Model(input_teacher, x_teacher)\n",
    "\n",
    "# NUMERIC LAYER\n",
    "input_teacher = Input(shape=(2,))\n",
    "x_numeric = Dense(16, activation='relu')(input_teacher)\n",
    "numeric_model = Model(input_teacher, x_numeric)\n",
    "\n",
    "\n",
    "combined = concatenate([lstm_essay_model.output, \n",
    "                        state_model.output,\n",
    "                        grade_model.output,\n",
    "                        category_model.output,\n",
    "                        sub_category_model.output,\n",
    "                        teacher_model.output,\n",
    "                        numeric_model.output\n",
    "                       ])\n",
    "\n",
    "\n",
    "x_combined = Dense(128, activation='relu')(combined)\n",
    "x_combined = Dropout(0.5)(x_combined)\n",
    "\n",
    "x_combined = Dense(128, activation='relu')(combined)\n",
    "x_combined = Dropout(0.5)(x_combined)\n",
    "\n",
    "x_combined = Dense(128, activation='relu')(combined)\n",
    "x_combined = Dense(1, activation=\"sigmoid\")(x_combined)\n",
    "\n",
    "final_model = Model(inputs=[lstm_essay_model.input, \n",
    "                            state_model.input,\n",
    "                            grade_model.input,\n",
    "                            category_model.input,\n",
    "                            sub_category_model.input,\n",
    "                            teacher_model.input,\n",
    "                            numeric_model.input], \n",
    "                    outputs = x_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_15 (InputLayer)           (None, 320)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_16 (InputLayer)           (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_17 (InputLayer)           (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_18 (InputLayer)           (None, 3)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_19 (InputLayer)           (None, 3)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_20 (InputLayer)           (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_13 (Embedding)        (None, 320, 300)     15589500    input_15[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "embedding_14 (Embedding)        (None, 1, 50)        2600        input_16[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "embedding_15 (Embedding)        (None, 1, 50)        250         input_17[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "embedding_16 (Embedding)        (None, 3, 50)        500         input_18[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "embedding_17 (Embedding)        (None, 3, 50)        1550        input_19[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "embedding_18 (Embedding)        (None, 1, 50)        300         input_20[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "input_21 (InputLayer)           (None, 2)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_3 (LSTM)                   (None, 10)           12440       embedding_13[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "flatten_11 (Flatten)            (None, 50)           0           embedding_14[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "flatten_12 (Flatten)            (None, 50)           0           embedding_15[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "flatten_13 (Flatten)            (None, 150)          0           embedding_16[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "flatten_14 (Flatten)            (None, 150)          0           embedding_17[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "flatten_15 (Flatten)            (None, 50)           0           embedding_18[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_11 (Dense)                (None, 16)           48          input_21[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 476)          0           lstm_3[0][0]                     \n",
      "                                                                 flatten_11[0][0]                 \n",
      "                                                                 flatten_12[0][0]                 \n",
      "                                                                 flatten_13[0][0]                 \n",
      "                                                                 flatten_14[0][0]                 \n",
      "                                                                 flatten_15[0][0]                 \n",
      "                                                                 dense_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_14 (Dense)                (None, 128)          61056       concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_15 (Dense)                (None, 1)            129         dense_14[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 15,668,373\n",
      "Trainable params: 78,873\n",
      "Non-trainable params: 15,589,500\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "final_model.compile(loss = 'binary_crossentropy', metrics = ['accuracy'], optimizer = Adadelta())\n",
    "final_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input_length = 10000\n",
    "\n",
    "input_train = [  np.array(X_padded_essays_train)[:test_input_length], \n",
    "                 np.array(X_state_train)[:test_input_length],\n",
    "                 np.array(X_grade_train)[:test_input_length],\n",
    "                 np.array(X_category_train)[:test_input_length],\n",
    "                 np.array(X_sub_category_train)[:test_input_length],\n",
    "                 np.array(X_teacher_train)[:test_input_length],\n",
    "                 np.array(X_train_numeric)[:test_input_length],\n",
    "                ]\n",
    "\n",
    "\n",
    "input_cv = [  np.array(X_padded_essays_cv)[:test_input_length], \n",
    "                 np.array(X_state_cv)[:test_input_length],\n",
    "                 np.array(X_grade_cv)[:test_input_length],\n",
    "                 np.array(X_category_cv)[:test_input_length],\n",
    "                 np.array(X_sub_category_cv)[:test_input_length],\n",
    "                 np.array(X_teacher_cv)[:test_input_length],\n",
    "                 np.array(X_cv_numeric)[:test_input_length],\n",
    "                ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "10000/10000 [==============================] - 76s 8ms/step - loss: 2.0878 - acc: 0.7528\n",
      "Epoch 2/10\n",
      "10000/10000 [==============================] - 75s 8ms/step - loss: 1.3575 - acc: 0.7865\n",
      "Epoch 3/10\n",
      " 3968/10000 [==========>...................] - ETA: 43s - loss: 1.1351 - acc: 0.7873"
     ]
    }
   ],
   "source": [
    "final_model.fit(input_train, y_train[:test_input_length], \n",
    "         batch_size = 128, \n",
    "         epochs = 10,\n",
    "         verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.42306862662400596"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import auc, roc_curve\n",
    "\n",
    "y_cv_proba = final_model.predict(input_cv)\n",
    "test_fpr, test_tpr, te_thresholds = roc_curve(y_cv, y_cv_proba)\n",
    "auc(test_fpr, test_tpr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
       "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
       "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                tokenizer=None, use_idf=True, vocabulary=None)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer(analyzer = 'word')\n",
    "tfidf.fit(X_train['essay'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Boxplot of IDF values for essay words')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "sns.boxplot(tfidf.idf_)\n",
    "plt.title('Boxplot of IDF values for essay words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25th Percentile : 9.682605929807279\n",
      "75th Percentile : 11.697508950349542\n"
     ]
    }
   ],
   "source": [
    "print('25th Percentile :', np.percentile(tfidf.idf_, 25))\n",
    "print('75th Percentile :', np.percentile(tfidf.idf_, 75))\n",
    "\n",
    "idf_low_threshold = np.percentile(tfidf.idf_, 2.5)\n",
    "idf_high_threshold = np.percentile(tfidf.idf_, 97.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a25d80d47e204e5380579de28a293081",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm_notebook\n",
    "\n",
    "stop_words = []\n",
    "idf_words = tfidf.get_feature_names()\n",
    "\n",
    "for idf_value, word in tqdm_notebook(zip(tfidf.idf_, idf_words)):\n",
    "    if idf_value < idf_low_threshold or idf_value > idf_high_threshold :\n",
    "        stop_words.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1298 words removed after filtering, \n"
     ]
    }
   ],
   "source": [
    "print('%d words removed after filtering, ' % len(stop_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove low `idf_` words from essays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_low_idf_words(args):\n",
    "        essay = args[0]\n",
    "        pool_list = args[1]\n",
    "        row_id = args[2]\n",
    "        processed_essay = ' '.join(e for e in essay.split() if e not in stop_words)\n",
    "        if len(processed_essay) == 0:\n",
    "            processed_essay = '  '\n",
    "        pool_list.append([row_id, processed_essay])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd29e083e4714d46a5ce53574f410251",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=88490), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing\n",
    "import os\n",
    "\n",
    "manager = multiprocessing.Manager()\n",
    "processed_essay = manager.list()\n",
    "args = [(essay, processed_essay, row_id) for essay, row_id in zip(X_train['essay'].values, X_train.index)]\n",
    "with multiprocessing.Pool(os.cpu_count()) as p:\n",
    "    r = list(tqdm_notebook(p.imap(remove_low_idf_words, args), total=X_train.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_essay_train = pd.DataFrame(processed_essay[:88600], columns = ['id', 'filtered_essay'])\n",
    "filtered_essay_train.index = filtered_essay_train['id']\n",
    "filtered_essay_train = filtered_essay_train.drop(['id'], axis = 1)\n",
    "X_train['id'] = X_train.index\n",
    "X_train = X_train.merge(filtered_essay_train, on = 'id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af8d72570c634ae19a92be930f043f53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=9833), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "manager = multiprocessing.Manager()\n",
    "processed_essay_cv = manager.list()\n",
    "args = [(essay, processed_essay_cv, row_id) for essay, row_id in zip(X_cv['essay'].values, X_cv.index)]\n",
    "with multiprocessing.Pool(os.cpu_count()) as p:\n",
    "    r = list(tqdm_notebook(p.imap(remove_low_idf_words, args), total=X_cv.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_essay_cv = pd.DataFrame(processed_essay_cv[:88600], columns = ['id', 'filtered_essay'])\n",
    "filtered_essay_cv.index = filtered_essay_cv['id']\n",
    "filtered_essay_cv = filtered_essay_cv.drop(['id'], axis = 1)\n",
    "X_cv['id'] = X_cv.index\n",
    "X_cv = X_cv.merge(filtered_essay_cv, on = 'id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "924d967eeebc40e2a3c1079712cd6df2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=10925), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "manager = multiprocessing.Manager()\n",
    "processed_essay_test = manager.list()\n",
    "args = [(essay, processed_essay_test, row_id) for essay, row_id in zip(X_test['essay'].values, X_test.index)]\n",
    "with multiprocessing.Pool(os.cpu_count()) as p:\n",
    "    r = list(tqdm_notebook(p.imap(remove_low_idf_words, args), total=X_test.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anirudhshenoy92/.local/lib/python3.5/site-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "filtered_essay_test = pd.DataFrame(processed_essay_test[:88600], columns = ['id', 'filtered_essay'])\n",
    "filtered_essay_test.index = filtered_essay_test['id']\n",
    "filtered_essay_test = filtered_essay_test.drop(['id'], axis = 1)\n",
    "X_test['id'] = X_test.index\n",
    "X_test = X_test.merge(filtered_essay_test, on = 'id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('glove_vectors', 'rb') as f:\n",
    "    model = pickle.load(f)\n",
    "    glove_words = set(model.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "tokenizer = Tokenizer(num_words=5000)\n",
    "X_train_essays = X_train['filtered_essay'].values\n",
    "\n",
    "tokenizer.fit_on_texts(X_train_essays)\n",
    "\n",
    "train_sequences = tokenizer.texts_to_sequences(X_train_essays)\n",
    "cv_sequences = tokenizer.texts_to_sequences(X_cv['filtered_essay'].values)\n",
    "test_sequences = tokenizer.texts_to_sequences(X_test['filtered_essay'].values)\n",
    "\n",
    "\n",
    "MAX_SEQUENCE_LENGTH = np.max([len(essay) for essay in train_sequences])\n",
    "\n",
    "X_padded_essays_train = pad_sequences(train_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "X_padded_essays_cv = pad_sequences(cv_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "X_padded_essays_test = pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11148341bbde4efe930ee3e3f2c92fda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=50666), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm_notebook\n",
    "GLOVE_VECTOR_DIMENSION = 300\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, GLOVE_VECTOR_DIMENSION))\n",
    "for word, i in tqdm_notebook(word_index.items()):\n",
    "    embedding_vector = model.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the LSTM Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_school_state = X_train['school_state'].values\n",
    "state_vocab = list(set(state for state in X_train_school_state ))\n",
    "vocab_size = len(state_vocab)\n",
    "\n",
    "X_state_train = [one_hot(state, vocab_size) for state in X_train_school_state]\n",
    "X_state_cv = [one_hot(state, vocab_size) for state in X_cv['school_state'].values]\n",
    "X_state_test = [one_hot(state, vocab_size) for state in X_test['school_state'].values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_project_grade_category = X_train['project_grade_category'].values\n",
    "grade_vocab = list(set(grade for grade in X_train_project_grade_category ))\n",
    "vocab_size = len(grade_vocab)\n",
    "\n",
    "X_grade_train = [one_hot(grade, vocab_size, filters = '') for grade in X_train_project_grade_category]\n",
    "X_grade_cv = [one_hot(grade, vocab_size, filters = '') for grade in X_cv['project_grade_category'].values]\n",
    "X_grade_test = [one_hot(grade, vocab_size, filters = '') for grade in X_test['project_grade_category'].values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_categories = X_train['clean_categories'].values\n",
    "category_vocab = list(set(category for categories in X_train_categories for category in categories.split()))\n",
    "vocab_size = len(category_vocab)\n",
    "\n",
    "X_category_train = [one_hot(category, vocab_size, filters = '') for category in X_train_categories]\n",
    "X_category_cv = [one_hot(category, vocab_size, filters = '') for category in X_cv['clean_categories'].values]\n",
    "X_category_test = [one_hot(category, vocab_size, filters = '') for category in X_test['clean_categories'].values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH_CATEGORY = np.max([len(essay) for essay in X_category_train])\n",
    "\n",
    "X_category_train = pad_sequences(X_category_train, maxlen=MAX_SEQUENCE_LENGTH_CATEGORY)\n",
    "X_category_cv = pad_sequences(X_category_cv, maxlen=MAX_SEQUENCE_LENGTH_CATEGORY)\n",
    "X_category_test = pad_sequences(X_category_test, maxlen=MAX_SEQUENCE_LENGTH_CATEGORY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_sub_categories = X_train['clean_subcategories'].values\n",
    "sub_category_vocab = list(set(category for categories in X_train_sub_categories for category in categories.split()))\n",
    "vocab_size = len(sub_category_vocab)\n",
    "\n",
    "X_sub_category_train = [one_hot(category, vocab_size, filters = '') for category in X_train_sub_categories]\n",
    "X_sub_category_cv = [one_hot(category, vocab_size, filters = '') for category in X_cv['clean_subcategories'].values]\n",
    "X_sub_category_test = [one_hot(category, vocab_size, filters = '') for category in X_test['clean_subcategories'].values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH_SUB_CATEGORY = np.max([len(essay) for essay in X_sub_category_train])\n",
    "\n",
    "X_sub_category_train = pad_sequences(X_sub_category_train, maxlen=MAX_SEQUENCE_LENGTH_SUB_CATEGORY)\n",
    "X_sub_category_cv = pad_sequences(X_sub_category_cv, maxlen=MAX_SEQUENCE_LENGTH_SUB_CATEGORY)\n",
    "X_sub_category_test = pad_sequences(X_sub_category_test, maxlen=MAX_SEQUENCE_LENGTH_SUB_CATEGORY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_teacher = X_train['teacher_prefix'].values\n",
    "teacher_vocab = list(set(category for categories in X_train_teacher for category in categories.split()))\n",
    "vocab_size = len(teacher_vocab)\n",
    "\n",
    "X_teacher_train = [one_hot(prefix, vocab_size, filters = '') for prefix in X_train_teacher]\n",
    "X_teacher_cv = [one_hot(prefix, vocab_size, filters = '') for prefix in X_cv['teacher_prefix'].values]\n",
    "X_teacher_test = [one_hot(prefix, vocab_size, filters = '') for prefix in X_test['teacher_prefix'].values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_numeric = X_train[['teacher_number_of_previously_posted_projects', 'price']].values\n",
    "X_cv_numeric = X_cv[['teacher_number_of_previously_posted_projects', 'price']].values\n",
    "X_test_numeric = X_test[['teacher_number_of_previously_posted_projects', 'price']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/anirudhshenoy92/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "# ESSAYS LAYER\n",
    "input_essays = Input(shape=(MAX_SEQUENCE_LENGTH,))\n",
    "x_essays = Embedding(len(word_index) + 1, \n",
    "              GLOVE_VECTOR_DIMENSION, \n",
    "              weights=[embedding_matrix],\n",
    "              input_length=MAX_SEQUENCE_LENGTH,\n",
    "              trainable = False)(input_essays)\n",
    "x_essays = LSTM(10)(x_essays)\n",
    "lstm_essay_model = Model(input_essays, x_essays)\n",
    "\n",
    "\n",
    "EMBEDDING_DIMENSION = 5\n",
    "\n",
    "# STATES LAYER\n",
    "input_states = Input(shape=(1,))\n",
    "x_state = Embedding(len(state_vocab) + 1, EMBEDDING_DIMENSION, input_length=1)(input_states)\n",
    "x_state = Flatten()(x_state)\n",
    "state_model = Model(input_states, x_state)\n",
    "\n",
    "# GRADE LAYER\n",
    "input_grades = Input(shape=(1,))\n",
    "x_grade = Embedding(len(grade_vocab) + 1, EMBEDDING_DIMENSION, input_length=1)(input_grades)\n",
    "x_grade = Flatten()(x_grade)\n",
    "grade_model = Model(input_grades, x_grade)\n",
    "\n",
    "# CATEGORY LAYER\n",
    "input_category = Input(shape=(MAX_SEQUENCE_LENGTH_CATEGORY,))\n",
    "x_category = Embedding(len(category_vocab) + 1, EMBEDDING_DIMENSION, input_length=MAX_SEQUENCE_LENGTH_CATEGORY)(input_category)\n",
    "x_category = Flatten()(x_category)\n",
    "category_model = Model(input_category, x_category)\n",
    "\n",
    "# SUB CATEGORY LAYER\n",
    "input_sub_category = Input(shape=(MAX_SEQUENCE_LENGTH_SUB_CATEGORY,))\n",
    "x_sub_category = Embedding(len(sub_category_vocab) + 1, EMBEDDING_DIMENSION, input_length=MAX_SEQUENCE_LENGTH_SUB_CATEGORY)(input_sub_category)\n",
    "x_sub_category = Flatten()(x_sub_category)\n",
    "sub_category_model = Model(input_sub_category, x_sub_category)\n",
    "\n",
    "# TEACHER PREFIX LAYER\n",
    "input_teacher = Input(shape=(1,))\n",
    "x_teacher = Embedding(len(teacher_vocab) + 1, EMBEDDING_DIMENSION, input_length=1)(input_teacher)\n",
    "x_teacher = Flatten()(x_teacher)\n",
    "teacher_model = Model(input_teacher, x_teacher)\n",
    "\n",
    "# NUMERIC LAYER\n",
    "input_teacher = Input(shape=(2,))\n",
    "x_numeric = Dense(16, activation='relu')(input_teacher)\n",
    "numeric_model = Model(input_teacher, x_numeric)\n",
    "\n",
    "\n",
    "combined = concatenate([lstm_essay_model.output, \n",
    "                        state_model.output,\n",
    "                        grade_model.output,\n",
    "                        category_model.output,\n",
    "                        sub_category_model.output,\n",
    "                        teacher_model.output,\n",
    "                        numeric_model.output\n",
    "                       ])\n",
    "\n",
    "\n",
    "x_combined = Dense(128, activation='relu')(combined)\n",
    "x_combined = Dropout(0.5)(x_combined)\n",
    "\n",
    "x_combined = Dense(128, activation='relu')(combined)\n",
    "x_combined = Dropout(0.5)(x_combined)\n",
    "\n",
    "x_combined = Dense(128, activation='relu')(combined)\n",
    "x_combined = Dense(1, activation=\"sigmoid\")(x_combined)\n",
    "\n",
    "final_model = Model(inputs=[lstm_essay_model.input, \n",
    "                            state_model.input,\n",
    "                            grade_model.input,\n",
    "                            category_model.input,\n",
    "                            sub_category_model.input,\n",
    "                            teacher_model.input,\n",
    "                            numeric_model.input], \n",
    "                    outputs = x_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 112)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            (None, 3)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_5 (InputLayer)            (None, 3)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_6 (InputLayer)            (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 112, 300)     15200100    input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 1, 5)         260         input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_3 (Embedding)         (None, 1, 5)         25          input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_4 (Embedding)         (None, 3, 5)         50          input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_5 (Embedding)         (None, 3, 5)         155         input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_6 (Embedding)         (None, 1, 5)         30          input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_7 (InputLayer)            (None, 2)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   (None, 10)           12440       embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 5)            0           embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 5)            0           embedding_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "flatten_3 (Flatten)             (None, 15)           0           embedding_4[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "flatten_4 (Flatten)             (None, 15)           0           embedding_5[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "flatten_5 (Flatten)             (None, 5)            0           embedding_6[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 16)           48          input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 71)           0           lstm_1[0][0]                     \n",
      "                                                                 flatten_1[0][0]                  \n",
      "                                                                 flatten_2[0][0]                  \n",
      "                                                                 flatten_3[0][0]                  \n",
      "                                                                 flatten_4[0][0]                  \n",
      "                                                                 flatten_5[0][0]                  \n",
      "                                                                 dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 128)          9216        concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 1)            129         dense_4[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 15,222,453\n",
      "Trainable params: 22,353\n",
      "Non-trainable params: 15,200,100\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "final_model.compile(loss = 'binary_crossentropy', metrics = ['accuracy'], optimizer = Adadelta())\n",
    "final_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input_length = 80000\n",
    "\n",
    "input_train = [  np.array(X_padded_essays_train)[:test_input_length], \n",
    "                 np.array(X_state_train)[:test_input_length],\n",
    "                 np.array(X_grade_train)[:test_input_length],\n",
    "                 np.array(X_category_train)[:test_input_length],\n",
    "                 np.array(X_sub_category_train)[:test_input_length],\n",
    "                 np.array(X_teacher_train)[:test_input_length],\n",
    "                 np.array(X_train_numeric)[:test_input_length],\n",
    "                ]\n",
    "\n",
    "\n",
    "input_cv = [  np.array(X_padded_essays_cv)[:test_input_length], \n",
    "                 np.array(X_state_cv)[:test_input_length],\n",
    "                 np.array(X_grade_cv)[:test_input_length],\n",
    "                 np.array(X_category_cv)[:test_input_length],\n",
    "                 np.array(X_sub_category_cv)[:test_input_length],\n",
    "                 np.array(X_teacher_cv)[:test_input_length],\n",
    "                 np.array(X_cv_numeric)[:test_input_length],\n",
    "                ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "80000/80000 [==============================] - 197s 2ms/step - loss: 0.8776 - acc: 0.8029\n",
      "Epoch 2/10\n",
      "80000/80000 [==============================] - 199s 2ms/step - loss: 0.9699 - acc: 0.7963\n",
      "Epoch 3/10\n",
      "80000/80000 [==============================] - 201s 3ms/step - loss: 1.0280 - acc: 0.7941\n",
      "Epoch 4/10\n",
      "80000/80000 [==============================] - 196s 2ms/step - loss: 1.1825 - acc: 0.7910\n",
      "Epoch 5/10\n",
      "80000/80000 [==============================] - 196s 2ms/step - loss: 1.3399 - acc: 0.7892\n",
      "Epoch 6/10\n",
      "80000/80000 [==============================] - 194s 2ms/step - loss: 1.4168 - acc: 0.7889\n",
      "Epoch 7/10\n",
      "80000/80000 [==============================] - 200s 2ms/step - loss: 1.4849 - acc: 0.7912\n",
      "Epoch 8/10\n",
      "80000/80000 [==============================] - 198s 2ms/step - loss: 1.5722 - acc: 0.7984\n",
      "Epoch 9/10\n",
      "69632/80000 [=========================>....] - ETA: 25s - loss: 1.6104 - acc: 0.7956"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-63-e319b4c6ecf6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m          \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m          \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m          verbose = 1)\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "final_model.fit(input_train, y_train[:test_input_length], \n",
    "         batch_size = 128, \n",
    "         epochs = 10,\n",
    "         verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4756647823894884"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import auc, roc_curve\n",
    "\n",
    "y_cv_proba = final_model.predict(input_cv)\n",
    "test_fpr, test_tpr, te_thresholds = roc_curve(y_cv, y_cv_proba)\n",
    "auc(test_fpr, test_tpr)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
